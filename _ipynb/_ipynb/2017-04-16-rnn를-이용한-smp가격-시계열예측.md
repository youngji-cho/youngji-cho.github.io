---
layout: post
title: "2017-04-16-RNN를-이용한-SMP가격-시계열예측"
tags:
    - python
    - notebook
------
title: "RNN를 이용한 SMP가격예측"
author: "youngji"
date: '2017-04-10'
--- 
본 글에서는 인공신경망 분석을 통한 SMP가격예측을 해볼것이다. 인공신경망은 입력충,출력충간의 네트워크 관계를 분석해서 최적의 결과를 예측한다.
입력충과 출력충간의 하나의 출력함수밖에 없으면 이를 단층 신경망이라고 한다. 입력충과 출력충간의 중간단계가 있다면 이러한 중간단계를
은닉충(hidden layer)라고 하며 이를 다층 신경망이라고 한다. 입력충과 출력충사이는 출력함수로 연결되어 있다. 출력함수의
계수(coefficient)를 "연결강도"라고 한다. 출력함수는 정말로 다양한 함수가 있지만, 가장 많이 사용되는 함수가 시그모이드
함수(sigmoid function)이다. 시그모이드 함수는 S자 형태의 비선형 함수이고 수식은 아래와 같다. $\lambda$는 함수의
경사도(gradient)를 의미한다.

$$ f(W)=\frac{1}{1+exp(\lambda(x))} $$

RNN(Recurrent Neural Network,순환인공신경망)은 이전의 결과를 반영한 신경망 분석이다. 때문에 시계열 분석에서 많이
쓰인다. 

**In [4]:**

{% highlight python %}
import pandas as pd 
import os
import matplotlib.pyplot as plt 
%matplotlib inline

os.chdir("/Users/youngji/Dropbox/data")
smp_price1=pd.read_csv("smp_price.csv",header=1,names=['육지(SMP)','제주(SMP)','SMP','BLMP'])
smp_price2=smp_price1.sort_index(axis=0, ascending=True)
smp_price3=smp_price2.set_index(keys=pd.period_range(start='2001-04',periods=smp_price2.shape[0],freq='M'))
smp_price4=smp_price3['SMP']

elec_supply1=pd.read_csv("elec_supply.csv",names=["기간","설비용량","공급능력","최대전력","공급예비력","공급예비율","기준시간"])
elec_supply2=elec_supply1.sort_values("기간", ascending=True)
elec_supply3=elec_supply2[elec_supply2.기간!='2002/08']
elec_supply4=elec_supply3.set_index(keys=pd.period_range(start='2003-01',periods=elec_supply3.shape[0],freq='M'))
elec_supply5=elec_supply4[['설비용량','공급능력','최대전력','공급예비력','공급예비율']]

fuel_cost1=pd.read_csv("fuel_cost.csv",header=0,names=["기간","원자력","유연탄","무연탄","유가","LNG가격"])
fuel_cost2=fuel_cost1.sort_values("기간", ascending=True)
fuel_cost3=fuel_cost2.set_index(keys=pd.period_range(start='2001-04',periods=fuel_cost2.shape[0],freq='M'))
fuel_cost4=fuel_cost3[['유가','LNG가격']]
fuel_cost4

elec_data=pd.concat([smp_price4,elec_supply5,fuel_cost4],axis=1)['2003-01':'2016-12']
elec_data['설비용량']=elec_data['설비용량'].str.replace(',', '') 
elec_data['공급능력']=elec_data['공급능력'].str.replace(',', '')
elec_data['최대전력']=elec_data['공급능력'].str.replace(',', '')
elec_data['공급예비력']=elec_data['공급예비력'].str.replace(',', '')
elec_data=elec_data.astype(float)
{% endhighlight %}

**In [5]:**

{% highlight python %}
import statsmodels.api as sm
from statsmodels.sandbox.regression.predstd import wls_prediction_std
smp_supply_lm=sm.OLS(elec_data['SMP'],elec_data['공급예비율'])
smp_supply_result=smp_supply_lm.fit()
elec_data.loc['2003-01':'2015-12']
{% endhighlight %}




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SMP</th>
      <th>설비용량</th>
      <th>공급능력</th>
      <th>최대전력</th>
      <th>공급예비력</th>
      <th>공급예비율</th>
      <th>유가</th>
      <th>LNG가격</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2003-01</th>
      <td>73.52</td>
      <td>5380.0</td>
      <td>5248.0</td>
      <td>5248.0</td>
      <td>743.0</td>
      <td>17.0</td>
      <td>60.94</td>
      <td>61.64</td>
    </tr>
    <tr>
      <th>2003-02</th>
      <td>74.70</td>
      <td>5515.0</td>
      <td>5080.0</td>
      <td>5080.0</td>
      <td>723.0</td>
      <td>17.0</td>
      <td>60.88</td>
      <td>59.95</td>
    </tr>
    <tr>
      <th>2003-03</th>
      <td>104.52</td>
      <td>5515.0</td>
      <td>5171.0</td>
      <td>5171.0</td>
      <td>901.0</td>
      <td>21.0</td>
      <td>65.34</td>
      <td>58.34</td>
    </tr>
    <tr>
      <th>2003-04</th>
      <td>75.70</td>
      <td>5515.0</td>
      <td>5004.0</td>
      <td>5004.0</td>
      <td>920.0</td>
      <td>23.0</td>
      <td>68.92</td>
      <td>57.94</td>
    </tr>
    <tr>
      <th>2003-05</th>
      <td>119.89</td>
      <td>5562.0</td>
      <td>4729.0</td>
      <td>4729.0</td>
      <td>661.0</td>
      <td>16.0</td>
      <td>70.77</td>
      <td>57.60</td>
    </tr>
    <tr>
      <th>2003-06</th>
      <td>117.09</td>
      <td>5562.0</td>
      <td>5161.0</td>
      <td>5161.0</td>
      <td>957.0</td>
      <td>23.0</td>
      <td>67.40</td>
      <td>59.04</td>
    </tr>
    <tr>
      <th>2003-07</th>
      <td>160.94</td>
      <td>5607.0</td>
      <td>5581.0</td>
      <td>5581.0</td>
      <td>1278.0</td>
      <td>30.0</td>
      <td>65.86</td>
      <td>58.91</td>
    </tr>
    <tr>
      <th>2003-08</th>
      <td>154.19</td>
      <td>5608.0</td>
      <td>5549.0</td>
      <td>5549.0</td>
      <td>810.0</td>
      <td>17.0</td>
      <td>63.20</td>
      <td>55.22</td>
    </tr>
    <tr>
      <th>2003-09</th>
      <td>128.60</td>
      <td>5608.0</td>
      <td>5264.0</td>
      <td>5264.0</td>
      <td>776.0</td>
      <td>17.0</td>
      <td>60.92</td>
      <td>60.13</td>
    </tr>
    <tr>
      <th>2003-10</th>
      <td>88.59</td>
      <td>5605.0</td>
      <td>5303.0</td>
      <td>5303.0</td>
      <td>1056.0</td>
      <td>25.0</td>
      <td>60.17</td>
      <td>60.42</td>
    </tr>
    <tr>
      <th>2003-11</th>
      <td>71.73</td>
      <td>5605.0</td>
      <td>5231.0</td>
      <td>5231.0</td>
      <td>849.0</td>
      <td>19.0</td>
      <td>59.48</td>
      <td>58.47</td>
    </tr>
    <tr>
      <th>2003-12</th>
      <td>56.28</td>
      <td>5605.0</td>
      <td>5443.0</td>
      <td>5443.0</td>
      <td>830.0</td>
      <td>18.0</td>
      <td>60.13</td>
      <td>59.28</td>
    </tr>
    <tr>
      <th>2004-01</th>
      <td>54.11</td>
      <td>5599.0</td>
      <td>5308.0</td>
      <td>5308.0</td>
      <td>680.0</td>
      <td>15.0</td>
      <td>62.51</td>
      <td>57.46</td>
    </tr>
    <tr>
      <th>2004-02</th>
      <td>55.81</td>
      <td>5599.0</td>
      <td>5177.0</td>
      <td>5177.0</td>
      <td>539.0</td>
      <td>12.0</td>
      <td>63.61</td>
      <td>59.24</td>
    </tr>
    <tr>
      <th>2004-03</th>
      <td>63.60</td>
      <td>5649.0</td>
      <td>5002.0</td>
      <td>5002.0</td>
      <td>456.0</td>
      <td>10.0</td>
      <td>64.16</td>
      <td>59.33</td>
    </tr>
    <tr>
      <th>2004-04</th>
      <td>81.09</td>
      <td>5698.0</td>
      <td>5087.0</td>
      <td>5087.0</td>
      <td>739.0</td>
      <td>17.0</td>
      <td>63.62</td>
      <td>56.04</td>
    </tr>
    <tr>
      <th>2004-05</th>
      <td>89.63</td>
      <td>5698.0</td>
      <td>5260.0</td>
      <td>5260.0</td>
      <td>1023.0</td>
      <td>24.0</td>
      <td>64.07</td>
      <td>55.89</td>
    </tr>
    <tr>
      <th>2004-06</th>
      <td>97.84</td>
      <td>5698.0</td>
      <td>5132.0</td>
      <td>5132.0</td>
      <td>539.0</td>
      <td>12.0</td>
      <td>64.93</td>
      <td>56.03</td>
    </tr>
    <tr>
      <th>2004-07</th>
      <td>167.37</td>
      <td>5913.0</td>
      <td>5753.0</td>
      <td>5753.0</td>
      <td>626.0</td>
      <td>12.0</td>
      <td>68.76</td>
      <td>58.28</td>
    </tr>
    <tr>
      <th>2004-08</th>
      <td>116.77</td>
      <td>5913.0</td>
      <td>5839.0</td>
      <td>5839.0</td>
      <td>721.0</td>
      <td>14.0</td>
      <td>67.24</td>
      <td>60.17</td>
    </tr>
    <tr>
      <th>2004-09</th>
      <td>131.43</td>
      <td>5913.0</td>
      <td>5525.0</td>
      <td>5525.0</td>
      <td>866.0</td>
      <td>19.0</td>
      <td>67.77</td>
      <td>65.27</td>
    </tr>
    <tr>
      <th>2004-10</th>
      <td>146.64</td>
      <td>5913.0</td>
      <td>5156.0</td>
      <td>5156.0</td>
      <td>728.0</td>
      <td>17.0</td>
      <td>69.96</td>
      <td>65.32</td>
    </tr>
    <tr>
      <th>2004-11</th>
      <td>165.46</td>
      <td>5994.0</td>
      <td>5778.0</td>
      <td>5778.0</td>
      <td>1154.0</td>
      <td>25.0</td>
      <td>69.51</td>
      <td>67.72</td>
    </tr>
    <tr>
      <th>2004-12</th>
      <td>149.90</td>
      <td>5994.0</td>
      <td>5923.0</td>
      <td>5923.0</td>
      <td>1122.0</td>
      <td>23.0</td>
      <td>76.43</td>
      <td>71.52</td>
    </tr>
    <tr>
      <th>2005-01</th>
      <td>144.10</td>
      <td>5996.0</td>
      <td>5498.0</td>
      <td>5498.0</td>
      <td>530.0</td>
      <td>11.0</td>
      <td>72.96</td>
      <td>71.77</td>
    </tr>
    <tr>
      <th>2005-02</th>
      <td>95.46</td>
      <td>5996.0</td>
      <td>5598.0</td>
      <td>5598.0</td>
      <td>599.0</td>
      <td>12.0</td>
      <td>67.26</td>
      <td>71.48</td>
    </tr>
    <tr>
      <th>2005-03</th>
      <td>86.93</td>
      <td>5996.0</td>
      <td>5750.0</td>
      <td>5750.0</td>
      <td>841.0</td>
      <td>17.0</td>
      <td>63.14</td>
      <td>64.56</td>
    </tr>
    <tr>
      <th>2005-04</th>
      <td>52.13</td>
      <td>5999.0</td>
      <td>5658.0</td>
      <td>5658.0</td>
      <td>1131.0</td>
      <td>25.0</td>
      <td>63.49</td>
      <td>58.33</td>
    </tr>
    <tr>
      <th>2005-05</th>
      <td>55.00</td>
      <td>6099.0</td>
      <td>5646.0</td>
      <td>5646.0</td>
      <td>1204.0</td>
      <td>27.0</td>
      <td>68.80</td>
      <td>60.63</td>
    </tr>
    <tr>
      <th>2005-06</th>
      <td>60.34</td>
      <td>6099.0</td>
      <td>5521.0</td>
      <td>5521.0</td>
      <td>461.0</td>
      <td>9.0</td>
      <td>74.86</td>
      <td>60.40</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2013-07</th>
      <td>91.02</td>
      <td>8536.0</td>
      <td>7647.0</td>
      <td>7647.0</td>
      <td>436.0</td>
      <td>6.0</td>
      <td>229.13</td>
      <td>142.52</td>
    </tr>
    <tr>
      <th>2013-08</th>
      <td>161.11</td>
      <td>8614.0</td>
      <td>7873.0</td>
      <td>7873.0</td>
      <td>472.0</td>
      <td>6.0</td>
      <td>230.17</td>
      <td>142.45</td>
    </tr>
    <tr>
      <th>2013-09</th>
      <td>104.45</td>
      <td>8614.0</td>
      <td>7634.0</td>
      <td>7634.0</td>
      <td>865.0</td>
      <td>13.0</td>
      <td>226.03</td>
      <td>143.00</td>
    </tr>
    <tr>
      <th>2013-10</th>
      <td>105.74</td>
      <td>8614.0</td>
      <td>7007.0</td>
      <td>7007.0</td>
      <td>516.0</td>
      <td>8.0</td>
      <td>225.07</td>
      <td>140.14</td>
    </tr>
    <tr>
      <th>2013-11</th>
      <td>140.63</td>
      <td>8633.0</td>
      <td>7887.0</td>
      <td>7887.0</td>
      <td>618.0</td>
      <td>9.0</td>
      <td>223.47</td>
      <td>134.69</td>
    </tr>
    <tr>
      <th>2013-12</th>
      <td>139.04</td>
      <td>8697.0</td>
      <td>8127.0</td>
      <td>8127.0</td>
      <td>559.0</td>
      <td>7.0</td>
      <td>219.95</td>
      <td>143.04</td>
    </tr>
    <tr>
      <th>2014-01</th>
      <td>145.05</td>
      <td>8697.0</td>
      <td>8408.0</td>
      <td>8408.0</td>
      <td>733.0</td>
      <td>10.0</td>
      <td>217.75</td>
      <td>141.59</td>
    </tr>
    <tr>
      <th>2014-02</th>
      <td>133.78</td>
      <td>8717.0</td>
      <td>8333.0</td>
      <td>8333.0</td>
      <td>604.0</td>
      <td>8.0</td>
      <td>217.83</td>
      <td>150.74</td>
    </tr>
    <tr>
      <th>2014-03</th>
      <td>94.93</td>
      <td>8717.0</td>
      <td>7802.0</td>
      <td>7802.0</td>
      <td>585.0</td>
      <td>8.0</td>
      <td>219.37</td>
      <td>160.65</td>
    </tr>
    <tr>
      <th>2014-04</th>
      <td>75.04</td>
      <td>8733.0</td>
      <td>7129.0</td>
      <td>7129.0</td>
      <td>753.0</td>
      <td>12.0</td>
      <td>220.05</td>
      <td>150.24</td>
    </tr>
    <tr>
      <th>2014-05</th>
      <td>46.93</td>
      <td>8768.0</td>
      <td>7457.0</td>
      <td>7457.0</td>
      <td>715.0</td>
      <td>11.0</td>
      <td>218.11</td>
      <td>148.28</td>
    </tr>
    <tr>
      <th>2014-06</th>
      <td>46.31</td>
      <td>8795.0</td>
      <td>7683.0</td>
      <td>7683.0</td>
      <td>703.0</td>
      <td>10.0</td>
      <td>215.77</td>
      <td>144.18</td>
    </tr>
    <tr>
      <th>2014-07</th>
      <td>47.42</td>
      <td>8867.0</td>
      <td>8413.0</td>
      <td>8413.0</td>
      <td>807.0</td>
      <td>11.0</td>
      <td>199.05</td>
      <td>145.29</td>
    </tr>
    <tr>
      <th>2014-08</th>
      <td>58.74</td>
      <td>9016.0</td>
      <td>8223.0</td>
      <td>8223.0</td>
      <td>1154.0</td>
      <td>16.0</td>
      <td>177.90</td>
      <td>142.87</td>
    </tr>
    <tr>
      <th>2014-09</th>
      <td>64.29</td>
      <td>9020.0</td>
      <td>8104.0</td>
      <td>8104.0</td>
      <td>1163.0</td>
      <td>17.0</td>
      <td>216.44</td>
      <td>140.50</td>
    </tr>
    <tr>
      <th>2014-10</th>
      <td>69.92</td>
      <td>9120.0</td>
      <td>7875.0</td>
      <td>7875.0</td>
      <td>1394.0</td>
      <td>22.0</td>
      <td>216.00</td>
      <td>136.13</td>
    </tr>
    <tr>
      <th>2014-11</th>
      <td>86.40</td>
      <td>9285.0</td>
      <td>7790.0</td>
      <td>7790.0</td>
      <td>857.0</td>
      <td>12.0</td>
      <td>215.34</td>
      <td>140.10</td>
    </tr>
    <tr>
      <th>2014-12</th>
      <td>144.00</td>
      <td>9322.0</td>
      <td>8936.0</td>
      <td>8936.0</td>
      <td>920.0</td>
      <td>12.0</td>
      <td>206.71</td>
      <td>145.88</td>
    </tr>
    <tr>
      <th>2015-01</th>
      <td>83.56</td>
      <td>9283.0</td>
      <td>9050.0</td>
      <td>9050.0</td>
      <td>1271.0</td>
      <td>16.0</td>
      <td>204.22</td>
      <td>146.24</td>
    </tr>
    <tr>
      <th>2015-02</th>
      <td>112.01</td>
      <td>9410.0</td>
      <td>8793.0</td>
      <td>8793.0</td>
      <td>914.0</td>
      <td>12.0</td>
      <td>200.51</td>
      <td>130.40</td>
    </tr>
    <tr>
      <th>2015-03</th>
      <td>123.73</td>
      <td>9410.0</td>
      <td>8491.0</td>
      <td>8491.0</td>
      <td>954.0</td>
      <td>13.0</td>
      <td>174.03</td>
      <td>121.83</td>
    </tr>
    <tr>
      <th>2015-04</th>
      <td>150.39</td>
      <td>9537.0</td>
      <td>7964.0</td>
      <td>7964.0</td>
      <td>1305.0</td>
      <td>20.0</td>
      <td>161.36</td>
      <td>107.83</td>
    </tr>
    <tr>
      <th>2015-05</th>
      <td>155.80</td>
      <td>9568.0</td>
      <td>7687.0</td>
      <td>7687.0</td>
      <td>977.0</td>
      <td>15.0</td>
      <td>159.51</td>
      <td>101.24</td>
    </tr>
    <tr>
      <th>2015-06</th>
      <td>132.22</td>
      <td>9568.0</td>
      <td>8405.0</td>
      <td>8405.0</td>
      <td>1417.0</td>
      <td>20.0</td>
      <td>150.00</td>
      <td>90.39</td>
    </tr>
    <tr>
      <th>2015-07</th>
      <td>98.34</td>
      <td>9683.0</td>
      <td>8965.0</td>
      <td>8965.0</td>
      <td>1296.0</td>
      <td>17.0</td>
      <td>156.13</td>
      <td>89.89</td>
    </tr>
    <tr>
      <th>2015-08</th>
      <td>73.48</td>
      <td>9683.0</td>
      <td>8960.0</td>
      <td>8960.0</td>
      <td>1268.0</td>
      <td>17.0</td>
      <td>155.39</td>
      <td>93.06</td>
    </tr>
    <tr>
      <th>2015-09</th>
      <td>44.54</td>
      <td>9683.0</td>
      <td>8510.0</td>
      <td>8510.0</td>
      <td>1211.0</td>
      <td>17.0</td>
      <td>155.61</td>
      <td>97.80</td>
    </tr>
    <tr>
      <th>2015-10</th>
      <td>40.76</td>
      <td>9683.0</td>
      <td>8060.0</td>
      <td>8060.0</td>
      <td>1528.0</td>
      <td>23.0</td>
      <td>153.13</td>
      <td>100.90</td>
    </tr>
    <tr>
      <th>2015-11</th>
      <td>42.72</td>
      <td>9765.0</td>
      <td>8718.0</td>
      <td>8718.0</td>
      <td>1322.0</td>
      <td>18.0</td>
      <td>130.23</td>
      <td>100.21</td>
    </tr>
    <tr>
      <th>2015-12</th>
      <td>49.77</td>
      <td>9765.0</td>
      <td>8919.0</td>
      <td>8919.0</td>
      <td>1191.0</td>
      <td>15.0</td>
      <td>132.82</td>
      <td>101.25</td>
    </tr>
  </tbody>
</table>
<p>156 rows × 8 columns</p>
</div>


 
이제 본격적으로 RNN 함수를 도입할 것이다. SMP,유가,공급예비율간의 관계를 통해서 1년치 데이터를 예측해보고자 한다. 우선, 아래와 같이
데이터를 나누고 예측을 할 것이다.

|Train|Test|
| :-------: | :-------: |
| 2004-15년 데이터| 2016년 데이터| 

**In [6]:**

{% highlight python %}
train_set=elec_data.loc['2003-01':'2015-12'][['SMP','공급예비율',"유가"]]
test_set=elec_data.loc['2016-01':'2016-12'][['SMP','공급예비율',"유가"]]

import matplotlib
matplotlib.rc('font', family="AppleGothic")  
train_set.plot()
{% endhighlight %}




    <matplotlib.axes._subplots.AxesSubplot at 0x124180240>



 
![png](/figure) 


**In [7]:**

{% highlight python %}
import numpy as np
from keras.models import Sequential 
from keras.layers import SimpleRNN, Dense

np.random.seed(0) 
model = Sequential()
model.add(SimpleRNN(10, input_shape=(3, 1)))
model.add(Dense(1))
model.compile(loss='mse', optimizer='sgd')

test_set.shape
{% endhighlight %}




    (12, 3)



**In [None]:**

{% highlight python %}
history = model.fit(X_train, Y_train, nb_epoch=100, verbose=0)
plt.plot(history.history["loss"])
plt.title("Loss")
plt.show()

plt.plot(Y_train, 'ro-', label="target")
plt.plot(model.predict(X_train[:,:,:]), 'bs-', label="output")
plt.xlim(-0.5, 20.5)
plt.ylim(-1.1, 1.1)
plt.legend()
plt.title("After training")
plt.show()
{% endhighlight %}

**In [None]:**

{% highlight python %}

{% endhighlight %}
